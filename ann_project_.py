# -*- coding: utf-8 -*-
"""ANN Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ypZPeOcU99jEXcDmm20Xa-XkW8hHylxG

# Deep Learning Model
# All copyrights belongs to Â© Prakhar Dwivedi, Rishabh Kumar, Sagar Chandrekar, Madhav Krishna Vakka

### Importing the libraries
"""

import numpy as np
import pandas as pd
import tensorflow as tf

tf.__version__

"""## Part 1 - Data Preprocessing

### Importing the dataset
"""

dataset = pd.read_excel('Rough.xlsx')
X = dataset.iloc[0:, :3].values
y = dataset.iloc[:, -1].values

print(X)

print(y)

"""### Encoding categorical data

Label Encoding the "ToolUsed" column
"""

'''from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X[:, 2] = le.fit_transform(X[:, 2])'''

'''print(X)   # 1 -> Turning
           # 0 -> Knurling'''

"""One Hot Encoding the "Geography" column"""

'''from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')
X = np.array(ct.fit_transform(X))'''

'print(X)'

"""### Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 0)

"""### Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Part 2 - Building the ANN - Core Component of Deep Learning

### Initializing the ANN
"""

ann = tf.keras.models.Sequential()   # Making 'ann' variable into a object created from the instances of sequential class.

"""### Adding the input layer and the first hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation= 'relu'))   # From 'ann' we are creating dancing class.

# units in Dense() class creates number of hidden layers of neurons into it.
# Activation function in hidden layers should be Rectifier Activation Function. i.e. 'relu'

"""### Adding the second hidden layer"""

ann.add(tf.keras.layers.Dense(units= 6, activation= 'relu'))

"""### Adding the third hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation= 'relu'))

"""### Adding the fourth hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation= 'relu'))

"""### Adding the fifth hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation= 'relu'))

"""### Adding the sixth hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation= 'relu'))

"""### Adding the seventh hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation= 'relu'))

"""### Adding the output layer"""

ann.add(tf.keras.layers.Dense(units=1 ))    # This is output code. Since our output data is Binary hence 'units =1'

# If it was Classification Model (having multiple options as result we will use 'units =3' that too after one-hot encoding those outputs).

# In Output layer our 'activation function' would be 'sigmoid' because it gives result as well as probability.

"""## Part 3 - Training the ANN

### Compiling the ANN
"""

ann.compile(optimizer = 'adam' , loss ='mean_squared_error')

"""### Training the ANN on the Training set"""

ann.fit(X_train, y_train, batch_size = 32, epochs = 300)  # We are doing batch learning. epoch is learning strength

"""## Part 4 - Making the predictions and evaluating the model"""

'''Prediciting The Surface Roughness Value'''

y_pred = ann.predict(X_test)
np.set_printoptions(precision=2)
final = np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test),1)),1)
print(final)

"""### Predicting the Test set results

### Making the Confusion Matrix
"""